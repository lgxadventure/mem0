<p align="center">
  <img src="docs/images/mem0-bg.png" width="500px" alt="Mem0 Logo">
</p>

<p align="center">
  <a href="https://embedchain.ai/slack">
    <img src="https://img.shields.io/badge/slack-embedchain-brightgreen.svg?logo=slack" alt="Slack">
  </a>
  <a href="https://embedchain.ai/discord">
    <img src="https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat" alt="Discord">
  </a>
  <a href="https://twitter.com/mem0ai">
    <img src="https://img.shields.io/twitter/follow/mem0ai" alt="Twitter">
  </a>
</p>

# Mem0: The Memory Layer for Personalized AI

Mem0 provides a smart, self-improving memory layer for Large Language Models, enabling personalized AI experiences across applications.

> Note: The Mem0 repository now also includes the Embedchain project. We continue to maintain and support Embedchain â¤ï¸. You can find the Embedchain codebase in the [embedchain](https://github.com/mem0ai/mem0/tree/main/embedchain) directory.
## ğŸš€ Quick Start

### Installation
```bash
pip install mem0ai
```

### Basic Usage

```python
import os
from mem0 import Memory

os.environ["OPENAI_API_KEY"] = "xxx"

# Initialize Mem0
m = Memory()

# Store a memory from any unstructured text
result = m.add("I am working on improving my tennis skills. Suggest some online courses.", user_id="alice", metadata={"category": "hobbies"})
print(result)
# Created memory: Improving her tennis skills. Looking for online suggestions.

# Retrieve memories
all_memories = m.get_all()
print(all_memories)

# Search memories
related_memories = m.search(query="What are Alice's hobbies?", user_id="alice")
print(related_memories)

# Update a memory
result = m.update(memory_id="m1", data="Likes to play tennis on weekends")
print(result)

# Get memory history
history = m.history(memory_id="m1")
print(history)
```

## ğŸ”‘ Core Features

- **Multi-Level Memory**: User, Session, and AI Agent memory retention
- **Adaptive Personalization**: Continuous improvement based on interactions
- **Developer-Friendly API**: Simple integration into various applications
- **Cross-Platform Consistency**: Uniform behavior across devices
- **Managed Service**: Hassle-free hosted solution

## ğŸ“– Documentation

For detailed usage instructions and API reference, visit our documentation at [docs.mem0.ai](https://docs.mem0.ai).

## ğŸ”§ Advanced Usage

For production environments, you can use Qdrant as a vector store:

```python
from mem0 import Memory

config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
        }
    },
}

m = Memory.from_config(config)
```

## ğŸ—ºï¸ Roadmap

- Integration with various LLM providers
- Support for LLM frameworks
- Integration with AI Agents frameworks
- Customizable memory creation/update rules
- Hosted platform support

## ğŸ™‹â€â™‚ï¸ Support
Join our Slack or Discord community for support and discussions.
If you have any questions, feel free to reach out to us using one of the following methods:

- [Join our Discord](https://embedchain.ai/discord)
- [Join our Slack](https://embedchain.ai/slack)
- [Follow us on Twitter](https://twitter.com/mem0ai)
- [Email us](mailto:founders@mem0.ai)
Reviewer 1ï¼š
1ã€	In this study, the authors first measured the category similarity, but you used an autoencoder system for feature extraction. Why? We know that an autoencoder system can uncover the intrinsic features of images, but can these features fully reflect the discriminative nature of the categories? If they can, would it be better to use these features directly for scene classification? If they cannot, is it accurate to use such features for measuring category similarity?
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚ä¼—æ‰€å‘¨çŸ¥ï¼ŒåŸå§‹æ•°æ®ä¸­åŒ…å«å¤§é‡çš„å™ªå£°å’Œå†—ä½™ä¿¡æ¯ï¼Œå› æ­¤ï¼Œä½¿ç”¨Autoencoderé€šè¿‡æœ€å°åŒ–é‡æ„è¯¯å·®ï¼Œæå–é¥æ„Ÿåœºæ™¯çš„é™ç»´åçš„è¡¨å¾ã€‚æ­¤ç‰¹å¾å› æœªä½¿ç”¨æ ‡ç­¾è®­ç»ƒå’Œæœªæå–å¤šå°ºåº¦ã€å¤šæ·±åº¦ç‰¹å¾ã€‚å› æ­¤è¯¥ç‰¹å¾åˆ¤åˆ«æ€§ä¸å¼ºã€‚ä½†æ˜¯ï¼Œç”±æœ€å°åŒ–é‡æ„è¯¯å·®æå–çš„ç‰¹å¾ï¼Œæ•°æ®çš„ç‰¹å¾åˆ†å¸ƒé€¼è¿‘åŸå§‹æ•°æ®åˆ†å¸ƒã€‚ä½¿ç”¨å…¶ç‰¹å¾æ„é€ çš„ç±»åˆ«åˆ†å¸ƒæ›´ç¬¦åˆè‡ªç„¶è§„å¾‹ï¼Œå› æ­¤ï¼Œä½¿ç”¨å…¶åˆ†å¸ƒè®¡ç®—å‡ºçš„ç±»åˆ«å·®å¼‚æ€§æ˜¯è¾ƒä¸ºåˆç†å¹¶å……åˆ†çš„ã€‚
2ã€	Using multiple models for scene classification has been a commonly used approach in the past, and it can indeed improve the performance of the model to a certain extent. However, according to the principles of ensemble learning, ensemble methods are built upon the basis of complementary differences in information. In the research, many convolutional models are used for feature extraction and then integrated. How can the complementary differences between these models be ensured, and how can their effectiveness be proven?
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚é›†æˆå­¦ä¹ ç¡®å®å»ºç«‹åœ¨åŸºåˆ†ç±»å™¨å…·æœ‰å¤šæ ·æ€§å’Œäº’è¡¥æ€§çš„åŸºç¡€ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œç”±äºé¥æ„Ÿåœºæ™¯çš„å¤æ‚æ€§å’Œåœºæ™¯çš„å¤§å°å¼‚æ€§ï¼Œå› æ­¤éœ€è¦å¤šå°ºåº¦çš„å·ç§¯æ ¸å»æå–å¤šå°ºåº¦çš„ä¿¡æ¯ã€‚ç”±æ–‡ç« []è¯æ˜äº†ï¼Œå¤šå±‚å°å·ç§¯çš„ç¥ç»ç½‘ç»œï¼Œç­‰æ•ˆäºä¸€ä¸ªå¤§å·ç§¯çš„ç¥ç»ç½‘ç»œï¼Œå¹¶ä»ä¸­å¼•å…¥äº†æ›´å¤šçš„éçº¿æ€§ä¿¡æ¯ã€‚å› æ­¤æœ¬æ–‡ä½¿ç”¨å¤šä¸ªæ·±åº¦ä¸åŒçš„å°å·ç§¯ç¥ç»ç½‘ç»œæå–å¤šå°ºåº¦ä¿¡æ¯ï¼Œå…¶ç¬¦åˆé›†æˆå­¦ä¹ çš„åŸºåˆ†ç±»å™¨çš„å¤šæ ·æ€§ã€‚è€Œä¸ºä»€ä¹ˆå·ç§¯å±‚çš„æ·±åº¦æœ€å¤šæ˜¯åˆ°13å±‚ï¼Œæ­¤å†…å®¹å·²åœ¨æ–‡ç« çš„[]è¯´æ˜ï¼Œå› ä¸ºå¦‚æœç»§ç»­åŠ æ·±ç¥ç»ç½‘ç»œçš„æ·±åº¦ï¼Œå¯èƒ½ä¼šå‡ºç°æ¢¯åº¦çˆ†ç‚¸çš„æƒ…å½¢ã€‚
è€ŒåŒæ—¶ä¸ºäº†ä½“ç°é›†æˆå­¦ä¹ çš„äº’è¡¥æ€§ï¼Œæœ¬æ–‡åº”ç”¨ç‰¹å¾é€‰æ‹©çš„æ–¹å¼å»é™¤èåˆå¤šä¸ªåŸºåˆ†ç±»å™¨è½¯æ ‡ç­¾è¡¨å¾çš„å†—ä½™æ€§ä¿¡æ¯ï¼Œè€Œé€‰æ‹©åçš„è½¯æ ‡ç­¾ç»“æœæ˜¯ç›¸äº’è¡¥å……çš„ï¼Œä¼šä¿ƒè¿›åˆ¤åˆ«æ€§ç»“æœçš„ã€‚åŒæ—¶ï¼Œé€šè¿‡æ¶ˆèå®éªŒ[],éªŒè¯äº†ç‰¹å¾æå–çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œåœºæ™¯åˆ†ç±»è¿‡ç¨‹ä¸­ä½¿ç”¨çš„åŸºåˆ†ç±»å™¨ä¸­æ•ˆæœæœ€å¥½çš„æ˜¯13å±‚å·ç§¯+3å±‚å…¨è¿æ¥çš„VGG16æ¨¡å‹ï¼Œå‚ç…§è¡¨[].[],[]å®éªŒç»“æœ,å¯ä»¥å‘ç°æœ¬æ–‡çš„æ¨¡å‹ä¼˜äºVGG-16æ¨¡å‹ã€‚
3ã€	In the analysis of the results, there was only a general comparison of the accuracy differences with other models, but no in-depth analysis of whether the model addressed the specific issues raised in this study, such as whether it improved the discrimination ability for similar categories and what impact it had on easily distinguishable categories.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚ç”±äºè®ºæ–‡ç¯‡å¹…æœ‰é™ï¼Œæœªåœ¨æ­£æ–‡ä¸­è¡¨è¿°æ¨¡å‹å¯¹ç›¸ä¼¼ç±»åˆ«çš„åˆ†ç±»çš„ä¿ƒè¿›ç»“æœã€‚åœ¨é™„å½•[],çš„æ··æ·†çŸ©é˜µå®éªŒä¸­ï¼Œè¯´æ˜äº†æœ¬æ–‡æ¨¡å‹å¯¹ç›¸ä¼¼ç±»åˆ«çš„åˆ¤åˆ«èƒ½åŠ›å¼ºäºè¡¨[],[],[]çš„æœ€ä¼˜ã€æ¬¡ä¼˜ç®—æ³•T-CNNï¼Œåœ¨å¯¹ä¸­ç­‰å°åŒºå’Œé«˜ç­‰å°åŒºçš„åˆ¤åˆ«è¯¯å·®ä¸‹é™[]%ã€‚
4ã€	Based on the results of this study, using such a complex system actually did not achieve higher accuracy compared to current advanced single network models. Then, what are the advantages of this method? It is recommended to test this method on more complex datasets or larger datasets to highlight its advantages over other models.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬æ–‡æ‰€åº”ç”¨çš„æ•°æ®é›†å‡æ˜¯è¿‘å‡ å¹´å¸¸è§çš„é¥æ„Ÿåœºæ™¯æ ‡å‡†ï¼Œè€Œæ›´å¤æ‚çš„é¥æ„Ÿæ•°æ®é›†æœªåœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼Œå› ä¸ºç»è´¹ã€è®¾å¤‡æœ‰é™ï¼Œæ— æ³•é‡‡é›†æ›´ä¸ºå¤æ‚çš„é¥æ„Ÿæ•°æ®ã€‚è‹¥revieweræœ‰æ›´å¥½çš„æ•°æ®é›†ï¼Œæ¬¢è¿åˆ†äº«ã€‚åŒæ—¶ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œå¯¹äºåœºæ™¯åˆ†ç±»ä»»åŠ¡ï¼Œç²¾åº¦æå‡çš„éš¾åº¦ä¸ç²¾åº¦çš„å¤§å°æˆæ­£ç›¸å…³å…³ç³»ï¼Œåœ¨å¦‚ä»Šçš„é¥æ„Ÿåœºæ™¯åˆ†ç±»ç²¾åº¦éš¾ä»¥æå‡çš„æƒ…å†µä¸‹ï¼Œéœ€è¦æ›´å¤æ‚çš„æ¨¡å‹å®ç°ç²¾åº¦çš„æå‡ã€‚
Reviewer 2ï¼š
1ã€	While the paper mentions the application of ensemble learning and deep learning in remote sensing scene classification in the introduction, it can further elaborate on the specific shortcomings of existing methods and how this study specifically improves upon these deficiencies. For instance, the paper mentions that existing deep models often overlook the intrinsic connections between difficult-to-distinguish categories when dealing with class distributions, but the literature review section is insufficient. Additional related works could be included to enhance the persuasiveness of the article and the perceived difficulty of the work presented.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚


2ã€	The paper proposes a deep ensemble learning model based on class distribution information, which includes two main modules: class distribution information extraction and scene classification. The article employs 13 convolutional layers and 3 fully connected layers. In ensemble learning, this is a variable hyperparameter, and the authors should demonstrate the optimal parameters of the method from theoretical and experimental perspectives or based on relevant literature.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚å…·ä½“çš„å¯ä»¥å‚ç…§Reviewer1çš„ç›¸å…³å†…å®¹ã€‚


3ã€	The paper introduces Î± as a balancing factor. When Î± reaches a certain threshold, the model tends to focus more on challenging instances, neglecting the rest and leading to a decline in overall performance. Figure 8 in the experimental results validates this point. However, is there a positive correlation between this factor and the amount of training data? As a hyperparameter, could the potential mechanism between the factor and the experiment be further explored from both theoretical and experimental standpoints?
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬ç ”ç©¶åœ¨æœªæ¥ä¼šè€ƒè™‘è¿™ä¸ªè¿›è¡Œè¿™ä¸ªå®éªŒã€‚


Reviewer 3ï¼š
1ã€	The base learners all employ 13 CNNs with similar structures. However, if the base classifiers are highly correlated, ensemble learning may fail to improve performance.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚è®ºæ–‡[]å·²ç»è¯´æ˜äº†ä½¿ç”¨çš„æ˜¯13ä¸ªæ·±åº¦ä¸åŒçš„ç¥ç»ç½‘ç»œï¼Œç»“æ„å¹¶ä¸ç›¸ä¼¼ï¼Œå¹¶ç›¸å…³çš„è§£é‡Šå¯å‚ç…§Reviewer1.3ã€‚
2ã€	I think the classification problem is relatively low difficulty and does not need to be handled with such a large model. And the method is not sufficiently correlated with geoscience and remote sensing.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚è¯·é—®é¥æ„Ÿåœºæ™¯åˆ†ç±»ä»»åŠ¡æ˜¯å¦å±äºé¥æ„Ÿé¢†åŸŸã€‚
3ã€	The modules in this paper such as autoencoder and CNN are basic backbones without optimization, and the authors did not discuss the layer and parameter selection for base learners.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚å‚è€ƒmethodéƒ¨åˆ†ï¼Œå·²ç»è¯´æ˜äº†åŸºåˆ†ç±»å™¨å’Œè‡ªç¼–ç å™¨çš„å±‚æ•°å’Œç»“æ„å‚æ•°ã€‚
4ã€	The experiment uses four evaluation metrics. For AA and KC, the authors only present the result of this paper without comparison with other methods. And there is no related statement in the text.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚ä½œä¸ºå¯¹æ¯”ç®—æ³•ï¼Œå¾ˆå¤šå¹¶æœªå¼€æºï¼Œå…¶è¿‡ç¨‹ä¸­å¹¶æœªä½¿ç”¨AAå’ŒKCï¼Œæœ¬æ–‡åº”ç”¨AAï¼ŒKCæ˜¯è¯æ˜æœ¬æ–‡çš„ç®—æ³•â€¦â€¦â€¦â€¦â€¦
5ã€ It is confusing that the analysis of CM is presented in the appendix.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚å¤§éƒ¨åˆ†çš„é¥æ„Ÿåœºæ™¯åˆ†ç±»ä»»åŠ¡å‡éœ€è¦æ··æ·†çŸ©é˜µçš„å®éªŒï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ†æã€‚
5ã€	The writing needs improvement. There are many spelling and grammar mistakes. For example, - Page 2 of 14, column left, line 31, â€œaâ€ should be â€œanâ€. - Page 2 of 14, column left, line 55, â€œOursâ€ shouldnâ€™t be capitalized. - Page 7 of 14, column left, line 59, â€œareâ€ should be â€œisâ€.
æ„Ÿè°¢ä½ çš„è¯„è®ºã€‚æˆ‘ä»¬ä¼šä»”ç»†æ ¡å‡†åˆç¨¿ï¼Œå¹¶æ”¹æ­£ä¸Šè¿°é”™è¯¯ã€‚
Reviewer 4ï¼š
1ã€ The technique novelties of this work are incremental and modest. The proposed Category Distribution Association learning scheme is in fact a variation of ensemble learning with adaboost, which has been extensively studied in the machine learning community for decades. Besides, the general idea to use ensemble learning to process the deep features for remote sensing scenes, have also been extensively studied in the past decade [1-2]
[1] Li, Erzhu, et al. "Integrating multilayer features of convolutional neural networks for remote sensing scene classification." IEEE Transactions on Geoscience and Remote Sensing 55.10 (2017): 5653-5665. [2] Hu, Fan, et al. "Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery." Remote Sensing 7.11 (2015): 14680-14707.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬æ–‡æ˜¯æ•´ä½“ä½¿ç”¨äº†é›†æˆå­¦ä¹ çš„æ¡†æ¶ï¼Œå¹¶é¦–æ¬¡å°†ç±»åˆ«åˆ†å¸ƒçš„å·®å¼‚ä¿¡æ¯å¼•å…¥è‡³boostingæ¡†æ¶ä¸­ï¼Œç¼“è§£äº†boostingè¿‡ç¨‹ä¸­ï¼Œé”™åˆ†ç±»æ ·æœ¬æå‡çš„ç¨‹åº¦ä¸€æ ·çš„é—®é¢˜ã€‚å…·ä½“è´¡çŒ®ç‚¹å¯å‚ç…§æ–‡ç« []

2ã€The compared references are old and out-of-date. More recent remote sensing scene classification methods in 2022-2023 [3-4] should be involved for comparison. [3] Wang, Junjie, et al. "Remote sensing scene classification via multi-stage self-guided separation network." IEEE Transactions on Geoscience and Remote Sensing (2023). [4] Yang, Yuqun, et al. "SAGN: Semantic-aware graph network for remote sensing scene classification." IEEE Transactions on Image Processing 32 (2023): 1011-1025.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬æ–‡å°†å¼•å…¥è¾ƒæ–°çš„å‚è€ƒæ–‡çŒ®å’Œæ¯”è¾ƒæ–¹æ³•ã€‚å…·ä½“å‚ç…§æ–°è®ºæ–‡[]ã€‚

3. The proposed method shows a clearly inferior performance when compared with the recent state-of-the-art. Besides, a naÃ¯ve use of vision Transformer model (e.g., ViT [5], Swin-T [6]), without using an additional complicated machine learning techniques, would show a significant better performance than the proposed method. [5] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). [6] Liu, Ze, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." Proceedings of the IEEE/CVF international conference on computer vision. 2021.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚å°½ç®¡è¿‘äº›å¹´æ¥ï¼Œtransformeræ¨¡å‹å¾—åˆ°äº†é•¿è¶³çš„å‘å±•ã€‚ä½†æ˜¯åœ¨é¥æ„Ÿé¢†åŸŸä¸­ä¾æ—§æœ‰å¾ˆå¤šSOTAæ˜¯åŸºäºCNNçš„æ–¹æ³•ã€‚

4. Could the authors make some clarification: whether the proposed method is end-to-end, or still need to first extract deep features from deep learning models and then fine-tune the extracted deep features on a classifier? If the later paradigm, it is relatively old and out-of-date. Again, devising the propose method into the deep models in a learnable and endto-end manner would be much more significant and practical, yielding better performance.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬æ–‡æ˜¯ä¸€ä¸ªtwo-stageçš„æ¨¡å‹ï¼Œæ˜¯å› ä¸ºä½¿ç”¨Autoencoderæå–ç±»åˆ«åˆ†å¸ƒè¡¨å¾ï¼Œå…¶å’Œstackingè¿‡ç¨‹çš„åŸºåˆ†ç±»å™¨ç¥ç»ç½‘ç»œï¼Œä»¥åŠå…ƒåˆ†ç±»å™¨æ— æ³•æ„å»ºåˆ°ä¸€ä¸ªç«¯å¯¹ç«¯çš„ç½‘ç»œä¸­ã€‚å°½ç®¡æœ¬æ–‡çš„æ¨¡å‹ä¸æ˜¯ç«¯å¯¹ç«¯çš„ç½‘ç»œï¼Œä½†æ˜¯å…¶æ€§èƒ½ä¸é€Šäºç«¯å¯¹ç«¯çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚åŒæ—¶æœªæ¥çš„å·¥ä½œä¸­ï¼Œä¹Ÿè€ƒè™‘å°†ç½‘ç»œè¿›è¡Œæ”¹è¿›ï¼Œä¿®æ”¹ä¸ºç«¯å¯¹ç«¯çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚

5. The ablation study lacks in some important aspect. Since the authors extract the deep features from each layer of the deep model, then an ablation on the impact from each layer deep features would be preferred.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æ­¤å¯å‚è€ƒReviewer1.3.æœ¬æ–‡åº”ç”¨äº†ç‰¹å¾é€‰æ‹©çš„æ–¹å¼ï¼ŒåŒæ—¶ä¹Ÿåšäº†å¯¹ç‰¹å¾é€‰æ‹©è¿‡ç¨‹çš„æ¶ˆèå®éªŒã€‚

6. From the reviewerâ€™s view, the performance of hand-crafted features in the tables could be removed, and again, to add more recent methodsâ€™ performance in 2022-2023.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚å…·ä½“è¯·å‚ç…§ä¸Šè¿°ç¬¬2æ¡ã€‚
7. After computing Eq.2-Eq.7, is there any visualization on the feature distribution and the weigh matrix for evaluation?
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚ç”±äºä½¿ç”¨è‡ªç¼–ç å™¨æå–çš„ç‰¹å¾å¤„äºè¾ƒé«˜ç»´æƒ…å†µå¹¶æœªå¼•å…¥æ ‡ç­¾çº¦æŸå…¶åˆ¤åˆ«æ€§ä¿¡æ¯ï¼Œä½¿ç”¨T-SNEç­‰å¯è§†åŒ–æ–¹æ³•ï¼Œä¼šå‡ºç°æ‹¥æŒ¤çš„æƒ…å†µï¼Œå› æ­¤å¹¶æœªå¯¹ç‰¹å¾åˆ†å¸ƒè¿›è¡Œå¯è§†åŒ–ã€‚
Other comments for improvement: 
8. Please provide more heatmaps of the proposed method? Can the features properly activate the key local regions of a remote sensing scene? 
9. Fig.5, 6 and 7 seems to be not necessary. 
10. Fig.8. Please provide higher-resolution figure, and enlarge the text font size
æ„Ÿè°¢ä¸Šè¿°çš„æ„è§ã€‚ä¸Šè¿°å†…å®¹ä¼šåœ¨æœªæ¥å‘å±•ä¸­ç»§ç»­ç ”ç©¶ã€‚
Reviewer 5ï¼š
1. One potential criticism is the lack of a thorough comparison with the most recent state-of-the-art methods. While the paper claims superiority over existing approaches, it would be beneficial to see a more detailed and direct comparison.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚æœ¬æ–‡å°†å¼•å…¥è¾ƒæ–°çš„å‚è€ƒæ–‡çŒ®å’Œæ¯”è¾ƒæ–¹æ³•ã€‚å…·ä½“å‚ç…§æ–°è®ºæ–‡[]ã€‚
2.The complexity of the proposed model might be a drawback in terms of computational efficiency. The paper does not address the computational cost and feasibility of applying this model to large-scale datasets.
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚ç”±äºæœ¬æ–‡æ˜¯stackingæ¶æ„ï¼Œå…¶ä¸­å„ä¸ªåŸºåˆ†ç±»å™¨å’Œç±»åˆ«åˆ†å¸ƒæå–æ¨¡å—æ˜¯å¯ä»¥å¹¶è¡Œè¿ç®—çš„ï¼Œé€‚ç”¨äºç›®å‰é¥æ„Ÿåˆ†ç±»é¢†åŸŸçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å…·ä½“çš„è®¡ç®—æˆæœ¬ï¼Œæœ¬æ–‡åœ¨3090ä¸­è¿ç®—
4. The paper could also benefit from a deeper discussion of the modelâ€™s limitations and potential failure cases. Understanding where the model might not perform well is crucial for practical applications. Additionally, the paper does not provide insights into how the model might be adapted or extended to other classification tasks beyond remote sensing scene classification." 
æ„Ÿè°¢ç²¾å½©çš„è¯„è®ºã€‚åœ¨å¸¸ç”¨çš„è‡ªç„¶å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸­ï¼Œå…¶ä¹Ÿå­˜åœ¨ç±»åˆ«ä¹‹é—´å­˜åœ¨æŒ‘æˆ˜æ€§ç±»å¯¹çš„é—®é¢˜ã€‚å› æ­¤å…¶ä¹Ÿé€‚ç”¨äºè‡ªç„¶å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

These criticisms are meant to be constructive.
5. Some Visualization Results could be provided to show how the deep feature learning could distinguish between categories with many similarities. 
6. Some cofusion matrix results can be be made to show the performanceã€‚
æ„Ÿè°¢ä¸Šè¿°çš„æ„è§ã€‚ä¸Šè¿°å†…å®¹ä¼šåœ¨æœªæ¥å‘å±•ä¸­ç»§ç»­ç ”ç©¶ã€‚
